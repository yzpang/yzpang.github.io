---
layout: archive
---

{{ content }}

<!-- <h3 class="archive__subtitle">{{ site.data.ui-text[site.locale].recent_posts | default: "Recent Posts" }}</h3> -->

<html>
<head>
<style>
p.xsmall {
    line-height: 1.55;
    font-size: 9.5pt;
    margin-left: 40px; 
}

p.small {
    line-height: 1.55;
    font-size: 11pt;
    margin-left: 40px; 
}

ul.small {
    line-height: 1.55;
    font-size: 11pt;
}

p.small2 {
    line-height: 2.00;
    font-size: 11.5pt;
    margin-left: 40px; 
}

p.medium {
    line-height: 1.55;
    font-size: 12.5pt;
    margin-left: 40px; 
}

p.big {
    line-height: 1.55;
}

p.noindent {
    line-height: 1.55;
    font-size: 12pt;
}


</style>
</head>
<body>









<p class="noindent">

Hello. I am a second-year Ph.D. student in the Courant Institute of Mathematical Sciences at New York University. I am a member of the <a href="https://wp.nyu.edu/ml2/people/" style="color: #57068C; text-decoration: none">Machine Learning for Language (MLÂ²) group</a> (subset of the <a href="https://wp.nyu.edu/cilvr/" style="color: #57068C; text-decoration: none">CILVR group</a>). I am advised by <a href="https://hhexiy.github.io" style="color: #57068C; text-decoration: none">Prof. He He</a> and <a href="http://www.kyunghyuncho.me" style="color: #57068C; text-decoration: none">Prof. Kyunghyun Cho</a>.

<br><br>

My research focuses on natural langauge processing and machine learning. Specifically, recent interests include text generation, neural machine translation, and structured prediction. 
<!-- Here are my interests. (1) <span style="font-weight:500">Structured prediction related algorithms</span>: (i) Approximate learning and inference; (ii) Multi-view learning and information theoretic co-training in structured setting (and relevant mutual information related projects); (iii) Computing distances among structured objects. (2) <span style="font-weight:500">Generation</span>: (i) Controlled text generation; (ii) Neural machine translation; (iii) Generating explanations for the purpose of interpreting neural codes or models.  -->

<br><br>

Prior to my Ph.D., I graduated from the University of Chicago in June 2019 (B.S. in mathematics and B.S. in computer science). In Chicago, my advisor was <a href="http://ttic.uchicago.edu/~kgimpel/" style="color: #004dd8; text-decoration: none">Prof. Kevin Gimpel</a> at Toyota Technological Institute at Chicago (TTIC) and the University of Chicago. In Summer 2020, I was a research intern at Google Research New York. 

</font>
<p>











<h3 style="font-weight: 500">Research</h3>





<p class="small">
<!-- </font> -->
<span style="line-height:170%"> 
<i>Primary</i> research subfields: <a style="color: #023DB4; text-decoration: none">generation</a>, <a style="color: #753DA4; text-decoration: none">machine translation</a>, <a style="color: #0A897D; text-decoration: none">structured prediction</a>, <a style="color: #22789D; text-decoration: none">others</a>.
</span>

<br>

<hr>


<!-- <p class="small">
<span style="line-height:170%"> <u> Preprints </u> </span>







<br><br>
 -->



<p class="small">
<span style="line-height:170%"> <u> Publications </u> </span>


<!-- <font size="3"> -->
<p class="small">
<span style="font-weight:500"> <a href="https://arxiv.org/abs/2009.07839" style="font-size: 12pt; color: #023DB4; text-decoration: none"> Text Generation by Learning from Off-Policy Demonstrations </a> <br> </span>
<span style="line-height:170%"> Richard Yuanzhe Pang, He He </span>
<br>
<span style="line-height:170%"> In <i>Proceedings of ICLR 2021</i> </span>
<br>
<span style="line-height:195%; font-size: 9pt"> tl;dr: a high-precision generation model to address the train/test objective mismatch and history mismatch </span>
<br>
[<a href="https://arxiv.org/pdf/2009.07839.pdf" style="color: #023DB4; text-decoration: none">paper (to be updated)</a>] [<a href="./research/#exactline-off-policy-gen" style="color: #023DB4; text-decoration: none">abstract</a>] [<a href="../misc-files/bibs/pang2020text.txt" style="color: #023DB4; text-decoration: none">bibtex</a>]


<p class="small">
<span style="font-weight:500"> <a href="https://arxiv.org/abs/1911.02891" style="font-size: 12pt; color: #0A897D; text-decoration: none"> Improving Joint Training of Inference Networks and Structured Prediction Energy Networks </a> <br> </span>
<span style="line-height:170%"> Lifu Tu, Richard Yuanzhe Pang, Kevin Gimpel</span>
<br>
<span style="line-height:170%"> In <i>Proceedings of EMNLP 2020 Workshop on Structured Prediction for NLP (SPNLP)</i>; spotlight paper </span>
<br>
<span style="line-height:195%; font-size: 9pt"> tl;dr: improving fast approximate+amortized inference for energy-based models in NLP structured prediction </span>
<br>
[<a href="https://arxiv.org/pdf/1911.02891.pdf" style="color: #0A897D; text-decoration: none">paper</a>] [<a href="./research/#exactline-spen" style="color: #0A897D; text-decoration: none">abstract</a>] [<a href="../misc-files/bibs/tu2019improving.txt" style="color: #0A897D; text-decoration: none">bibtex</a>]


<br>


<!-- <font size="3"> -->
<p class="small">
<span style="font-weight:500"> <a href="https://arxiv.org/abs/2002.02492" style="font-size: 12pt; color: #0A897D; text-decoration: none"> Consistency of a Recurrent Language Model With Respect to Incomplete Decoding </a> <br> </span>
<span style="line-height:170%"> Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, Kyunghyun Cho </span>
<br>
<span style="line-height:170%"> In <i>Proceedings of EMNLP 2020</i> </span>
<br>  
<span style="line-height:170%"> Also appearing in the non-archival <i>DeepMath 2020</i> </span>
<br>  
[<a href="https://arxiv.org/pdf/2002.02492.pdf" style="color: #0A897D; text-decoration: none">paper</a>] [<a href="./research/#exactline-emnlp20-a" style="color: #0A897D; text-decoration: none">abstract</a>] [<a href="../misc-files/bibs/welleck2020consistency.txt" style="color: #0A897D; text-decoration: none">bibtex</a>]


<br>


<p class="small">
<span style="font-weight:500"> <a href="https://arxiv.org/abs/2005.00850" style="font-size: 12pt; color: #753DA4; text-decoration: none"> ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation </a> <br> </span>
<span style="line-height:170%"> Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel</span>
<br>
<span style="line-height:170%"> In <i>Proceedings of ACL 2020</i> </span>
<br>
<span style="line-height:195%; font-size: 9pt"> tl;dr: a "soft" form of knowledge distillation for non-autoregressive MT </span>
<br>  
[<a href="https://arxiv.org/pdf/2005.00850.pdf" style="color: #753DA4; text-decoration: none">paper</a>] [<a href="./research/#exactline-acl20-a" style="color: #753DA4; text-decoration: none">abstract</a>] [<a href="../misc-files/bibs/tu2020engine.txt" style="color: #753DA4; text-decoration: none">bibtex</a>]


<br>


<p class="small">
<span style="font-weight:500"> <a href="https://arxiv.org/abs/2005.00628" style="font-size: 12pt; color: #22789D; text-decoration: none"> Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work? </a> <br> </span>
<span style="line-height:170%"> Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, Samuel R. Bowman</span>
<br>
<span style="line-height:170%"> In <i>Proceedings of ACL 2020</i> </span>
<br>  
[<a href="https://arxiv.org/pdf/2005.00628.pdf" style="color: #22789D; text-decoration: none">paper</a>] [<a href="./research/#exactline-acl20-b" style="color: #22789D; text-decoration: none">abstract</a>] [<a href="../misc-files/bibs/pruksachatkun2020intermediate.txt" style="color: #22789D; text-decoration: none">bibtex</a>]


<br>


<!-- <font size="3"> -->
<p class="small">
<span style="font-weight:500"> <a href="https://arxiv.org/abs/1810.11878" style="font-size: 12pt; color: #023DB4; text-decoration: none"> Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel Textual Transfer </a> <br> </span>
<span style="line-height:170%"> Richard Yuanzhe Pang, Kevin Gimpel </span>
<br>
<span style="line-height:170%"> In <i>Proceedings of EMNLP 2019 Workshop on Neural Generation and Translation (WNGT) </i> </span>
<br>
<span style="line-height:195%; font-size: 9pt"> tl;dr: proposing more dimensions for textual transfer evaluation metrics, and losses that target them </span>
<br>
[<a href="https://arxiv.org/pdf/1810.11878.pdf" style="color: #023DB4; text-decoration: none">paper</a>] [<a href="https://arxiv.org/pdf/1810.11878.pdf#page=11" style="color: #023DB4; text-decoration: none">supplementals</a>] [<a href="./research/#exactline-wngt19-a" style="color: #023DB4; text-decoration: none">abstract</a>] [<a href="../misc-files/pang+gimpel-textual-transfer-poster.pdf" style="color: #023DB4; text-decoration: none">poster</a>] [<a href="../misc-files/bibs/pang2018unsupervised.txt" style="color: #023DB4; text-decoration: none">bibtex</a>]


<br>


<p class="small">
<span style="font-weight:500"> <a href="https://arxiv.org/abs/1910.03747" style="font-size: 12pt; color: #023DB4; text-decoration: none"> The Daunting Task of Real-World Textual Style Transfer Auto-Evaluation </a> <br> </span>
<span style="line-height:170%"> Richard Yuanzhe Pang</span>
<br>
<span style="line-height:170%"> Extended abstract in <i>EMNLP 2019 Workshop on Neural Generation and Translation (WNGT)</i>; abstract in <i>Proceedings of the Workshop on Noisy User-generated Text (W-NUT)</i> 
<br>
<span style="line-height:195%; font-size: 9pt"> tl;dr: an opinion piece arguing that the research on textual style transfer and its evaluation are going astray </span>
<br>
[<a href="https://arxiv.org/pdf/1910.03747.pdf" style="color: #023DB4; text-decoration: none">paper</a>] [<a href="./research/#exactline-wngt19-b" style="color: #023DB4; text-decoration: none">abstract</a>] [<a href="../misc-files/pang-textual-transfer-problem-poster.pdf" style="color: #023DB4; text-decoration: none">poster</a>] [<a href="../misc-files/bibs/pang2019daunting.txt" style="color: #023DB4; text-decoration: none">bibtex</a>]


<br><br>

<p class="small">

My pages: [<a href="https://scholar.google.com/citations?user=zjpiT28AAAAJ&hl=en" style="color: #4C8BF5; text-decoration: none">google scholar</a>] [<a href="https://www.semanticscholar.org/author/Richard-Yuanzhe-Pang/46230016" style="color: #4C8BF5; text-decoration: none">semantic scholar</a>] [<a href="https://dblp.org/pid/250/9059.html" style="color: #4C8BF5; text-decoration: none">dblp</a>]


















<h3 style="font-weight: 500">Presentations</h3>
<hr>


<ul class="small">

<li> Invited talk on structured prediction; Bank of New York Mellon; September 2020 </li>   

<li> Talk titled <i>Text Generation by Offline RL</i>; Google Research New York; July 2020 </li>

<li> Poster presentation and lightning talk on <i>The Daunting Task of Real-World Textual Style Transfer Auto-Evaluation</i>; EMNLP 2019 WNGT workshop (poster) and W-NUT workshop (lightning talk) in Hong Kong, China; November 2019 [<a href="../misc-files/pang-textual-transfer-problem-poster.pdf" style="color: #023DB4; text-decoration: none">poster</a>]</li>



<li> Poster presentation on <i>Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel Textual Transfer</i>; EMNLP 2019 WNGT workshop in Hong Kong, China; November 2019 [<a href="../misc-files/pang+gimpel-textual-transfer-poster.pdf" style="color: #023DB4; text-decoration: none">poster</a>]</li>



<li> Poster presentation on <i>Learning Criteria and Evaluation Metrics for Textual Transfer between Non-Parallel Corpora</i>; NAACL 2019 NeuralGen workshop in Minneapolis, USA; June 2019 </li>



<li> Talk titled <i>Learning Approximate Inference Networks and Structured Prediction Energy Networks</i>; Midwest Speech and Language Days (MSLD) 2019 in Chicago, USA; May 2019 </li>



<li> Poster presentation on <i>Learning Criteria and Evaluation Metrics for Textual Transfer between Non-Parallel Corpora</i>; UChicago STEM Research Symposium in Chicago, USA; October 2018 </li>

</ul>















<h3 style="font-weight: 500">Teaching</h3>
<hr>

<p class="small">
<!-- <font size="3"> -->

<span style="line-height:300%"> <u>At New York University</u> </span>
<br>
Fall 2020, <i>Section Leader</i>, DS-GA 1008: Deep Learning (Cho, LeCun) [<a href="https://kyunghyuncho.me/courses/" style="color: #4d4d4d">syllabus]</a><br>

<br>

<span style="line-height:300%"> <u>At the University of Chicago</u> </span>
<br>
Spring 2017, <i>Course Assistant</i>, MATH 15910: Intro to Proofs in Analysis<br>
Winter 2017, <i>Course Assistant</i>, MATH 15910: Intro to Proofs in Analysis [<a href="../misc-files/math-15910-winter-2017-sol.html" style="color: #4d4d4d">sol]</a><br>
Winter 2017, <i>Grader</i>, CMSC 15200: Intro to Computer Science II<br>
Autumn 2016, <i>Teaching Assistant</i>, MATH 15300: Calculus III<br>

<!-- </font> -->
<p>












<!-- 
<h3 style="font-weight: 500">Relevant Coursework</h3>
<hr>

<p class="xsmall">
<span style="line-height:300%"> <u> At the University of Chicago (2015-2019) </u> </span>
<br>
CMSC 27230 - Honors Theory of Algorithms<br>
CMSC 25025 / STAT 37601 - Machine Learning and Large-Scale Data Analysis (grad level, Lafferty)<br>
CMSC 35400 / STAT 37710 - Machine Learning (grad level, Kondor)<br>
TTIC 31020 - Statistical Machine Learning (grad level, Shakhnarovich)<br>
TTIC 31190 - Natural Language Processing (grad level, Gimpel)<br>
TTIC 41000 - Spectral Techniques (grad level, Stratos)<br>
MATH 20300-20500 - Accelerated Real Analysis I, II, III<br>
MATH 20250, 25400-25500 - Abstract Linear Algebra; Abstract Algebra I, II<br>
BIOS 10602-10603 - Multiscale Modeling of Biological Systems I, II (computational biology)
 -->






<!-- 
<p>
 -->

<!-- <h3 style="font-weight: 500">Miscellaneous</h3>
<hr>

<p class="small">
I <a href="/writings/" style="color: #008016">write</a>. 
</p> -->




<br><br><br><br><br>



<p class="noindent" style="font-size: 11.5pt; margin-left: 0px; text-decoration: none">
<!-- <font size="3"> -->

Last updated: January 12, 2021. Get in touch at yzpang at _ dot edu (where _ is nyu or uchicago)!

<!-- </font> -->
</p>










</body>

{% for post in paginator.posts %}
  {% include archive-single.html %}
{% endfor %}

{% include paginator.html %}
